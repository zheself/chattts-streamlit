import streamlit as st
import streamlit_ace as st_ace
from io import StringIO
import contextlib
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader, random_split
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from datetime import datetime
from torch.nn import TransformerEncoder, TransformerEncoderLayer
import math



# å®šä¹‰ç¥ç»ç½‘ç»œç»“æ„
class ComplexNet(nn.Module):
    def __init__(self):
        super(ComplexNet, self).__init__()
        self.fc1 = nn.Linear(3, 64)  # è¾“å…¥å±‚åˆ°ç¬¬ä¸€ä¸ªéšè—å±‚
        self.fc2 = nn.Linear(64, 128)  # ç¬¬ä¸€ä¸ªéšè—å±‚åˆ°ç¬¬äºŒä¸ªéšè—å±‚
        self.fc3 = nn.Linear(128, 64)  # ç¬¬äºŒä¸ªéšè—å±‚åˆ°ç¬¬ä¸‰ä¸ªéšè—å±‚
        self.fc4 = nn.Linear(64, 2)  # ç¬¬ä¸‰ä¸ªéšè—å±‚åˆ°è¾“å‡ºå±‚

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.relu(self.fc3(x))
        x = self.fc4(x)
        return x


class ComplexLSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout_prob):
        super(ComplexLSTMModel, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        # å®šä¹‰LSTMå±‚
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout_prob)

        # æ·»åŠ Dropoutå±‚
        self.dropout = nn.Dropout(dropout_prob)

        # å®šä¹‰é¢å¤–çš„å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(hidden_dim, hidden_dim * 2)
        self.bn1 = nn.BatchNorm1d(hidden_dim * 2)  # æ‰¹é‡å½’ä¸€åŒ–å±‚
        self.fc2 = nn.Linear(hidden_dim * 2, output_dim)

    def forward(self, x):
        # åˆå§‹åŒ–éšè—çŠ¶æ€å’Œç»†èƒçŠ¶æ€
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)

        # å‰å‘ä¼ æ’­LSTM
        out, (hn, cn) = self.lstm(x, (h0, c0))

        # åªéœ€è¦LSTMæœ€åä¸€å±‚çš„è¾“å‡º
        out = out[:, -1, :]
        out = self.dropout(out)

        # é€šè¿‡å…¨è¿æ¥å±‚
        out = self.fc1(out)
        out = self.bn1(out)
        out = torch.relu(out)
        out = self.fc2(out)
        return out
# å®šä¹‰ä½ç½®ç¼–ç 
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, d_model)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

# å®šä¹‰Transformeræ¨¡å‹
class TransformerModel(nn.Module):
    def __init__(self, input_dim, seq_len, hidden_dim, output_dim, num_layers, dropout_prob):
        super(TransformerModel, self).__init__()
        self.hidden_dim = hidden_dim
        self.input_dim = input_dim
        self.pos_encoder = PositionalEncoding(hidden_dim, dropout_prob)

        # ç”±äº3æ˜¯è´¨æ•°ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©å°†è¾“å…¥ç‰¹å¾æŠ•å½±åˆ°æ›´é«˜çš„ç»´åº¦
        self.embedding = nn.Linear(input_dim, hidden_dim)

        encoder_layers = TransformerEncoderLayer(d_model=hidden_dim, nhead=8,
                                                 dim_feedforward=hidden_dim, dropout=dropout_prob)
        self.transformer_encoder = TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_layers)

        self.fc_out = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(dropout_prob)

    def forward(self, x):
        x = self.embedding(x)  # å°†è¾“å…¥ç‰¹å¾æŠ•å½±åˆ°æ›´é«˜çš„ç»´åº¦
        x = self.dropout(x)
        x = self.pos_encoder(x)
        x = x.permute(1, 0, 2)  # Transformer expects (Seq Len, Batch, Features)
        out = self.transformer_encoder(x)
        out = self.fc_out(out[-1, :, :])  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        return out


def evaluate_model(model, test_loader):
    model.eval()  # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    predictions = []
    actuals = []
    with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦
        for inputs, targets in test_loader:
            inputs, targets = inputs.to(device), targets.to(device)  # ç§»åŠ¨æ•°æ®åˆ°GPU
            outputs = model(inputs)
            predictions.append(outputs.cpu().numpy())
            actuals.append(targets.cpu().numpy())

    predictions = np.vstack(predictions)
    actuals = np.vstack(actuals)

    # åæ ‡å‡†åŒ–ä»¥æ¯”è¾ƒå®é™…å€¼
    predictions = scaler_y.inverse_transform(predictions)
    actuals = scaler_y.inverse_transform(actuals)

    # è®¡ç®—å‡æ–¹æ ¹è¯¯å·®
    mse = mean_squared_error(actuals, predictions)
    rmse = np.sqrt(mse)

    # è®¡ç®—å¹³å‡ç»å¯¹è¯¯å·®
    mae = mean_absolute_error(actuals, predictions)

    return rmse, mae
# åŠ è½½å’Œå‡†å¤‡æ•°æ®
data = pd.read_csv('C://Users//Administrator//Desktop//ChatTTS-main//pages//earthquake.csv')
data['Date'] = pd.to_datetime(data['Date'])
data['Time'] = pd.to_datetime(data['Time'], format='%H:%M:%S').dt.time
data['Timestamp'] = data.apply(lambda row: pd.Timestamp(f"{row['Date']} {row['Time']}"), axis=1)
data['Timestamp'] = data['Timestamp'].view('int64') // 10**9 #Unixæ—¶é—´æˆ³
features = data[['Timestamp', 'Longitude', 'Latitude']]
targets = data[['Depth', 'Magnitude']]
X_np=features.values
y_np=targets.values
scaler_x = MinMaxScaler()
scaler_y = MinMaxScaler()
scaler_x.fit(X_np)
scaler_y.fit(y_np)




device = torch.device("cpu")
# åˆå§‹åŒ–ç½‘ç»œ
model_BP = ComplexNet()
model_BP.to(device)
# åˆå§‹åŒ–æ¨¡å‹
input_dim = 3 # ç»´åº¦ã€ç»åº¦ã€æ—¶é—´
hidden_dim = 64
num_layers = 2
output_dim = 2 # éœ‡æ·±å’Œéœ‡çº§
dropout_prob = 0.5

model_lstm = ComplexLSTMModel(input_dim, hidden_dim, output_dim, num_layers, dropout_prob)

# å°†æ¨¡å‹ç§»åˆ°GPU
model_lstm = model_lstm.to(device)
# åˆå§‹åŒ–æ¨¡å‹å‚æ•°
input_dim = 3  # æ—¶é—´ã€ç»åº¦ã€çº¬åº¦
seq_len = 20   # åºåˆ—é•¿åº¦
hidden_dim = 64  # éšè—å±‚çš„ç»´åº¦
num_layers = 2  # Transformerå±‚çš„æ•°é‡
output_dim = 2  # éœ‡æ·±å’Œéœ‡çº§
dropout_prob = 0.5  # Dropoutæ¦‚ç‡

# åˆ›å»ºæ¨¡å‹
model_transformer = TransformerModel(input_dim, seq_len, hidden_dim, output_dim, num_layers, dropout_prob)

# å°†æ¨¡å‹ç§»åˆ°GPU
model_transformer = model_transformer.to(device)
# åŠ è½½æœ€ä½³æ¨¡å‹
model_BP.load_state_dict(torch.load('C://Users//Administrator//Desktop//ChatTTS-main//minloss_BP_model.pth', map_location=torch.device('cpu')))
model_BP.to(device)  # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
# åŠ è½½æœ€ä½³æ¨¡å‹
model_lstm.load_state_dict(torch.load('C://Users//Administrator//Desktop//ChatTTS-main//best_lstm_model.pth', map_location=torch.device('cpu')))
model_lstm.to(device)  # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
# åŠ è½½æœ€ä½³æ¨¡å‹
model_transformer.load_state_dict(torch.load('C://Users//Administrator//Desktop//ChatTTS-main//best_transformer_model.pth', map_location=torch.device('cpu')))
model_transformer.to(device)  # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š


def BP_forward(time, Latitude, Longitude):
    '''
    time:1965-01-02 13:44:18

    '''
    model=model_BP
    # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
    model.eval()
    time = pd.to_datetime(time)
    time = time.timestamp()  # Unixæ—¶é—´æˆ³
    features = np.array([[Latitude, Longitude, time]])
    X = torch.tensor(features, dtype=torch.float32)
    # ä¸è®¡ç®—æ¢¯åº¦
    with torch.no_grad():
        X = X.to(device)
        output = model(X)
    return output


def lstm_forward(time, Latitude, Longitude):
    model=model_lstm
    model.eval() # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    predictions = []
    time = pd.to_datetime(time)
    time=time.timestamp() #Unixæ—¶é—´æˆ³
    features=np.array([[Latitude, Longitude, time]])
    X = scaler_x.transform(features)
    X = torch.tensor(X, dtype=torch.float32)
    X = X.reshape(1,1,3)
    with torch.no_grad(): # ä¸è®¡ç®—æ¢¯åº¦
        X = X.to(device)
        outputs = model(X)
        predictions.append(outputs.cpu().numpy())

    # åæ ‡å‡†åŒ–ä»¥æ¯”è¾ƒå®é™…å€¼

    predictions = np.vstack(predictions)
    predictions = scaler_y.inverse_transform(predictions)
    return predictions
def transformer_forward(time, Latitude, Longitude):
    model=model_transformer
    model.eval() # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    predictions = []
    time = pd.to_datetime(time)
    time=time.timestamp() #Unixæ—¶é—´æˆ³
    features=np.array([[Latitude, Longitude, time]])
    X = scaler_x.transform(features)
    X = torch.tensor(X, dtype=torch.float32)
    X = X.reshape(1,1,3)
    with torch.no_grad(): # ä¸è®¡ç®—æ¢¯åº¦
        X = X.to(device)
        outputs = model(X)
        predictions.append(outputs.cpu().numpy())

    # åæ ‡å‡†åŒ–ä»¥æ¯”è¾ƒå®é™…å€¼

    predictions = np.vstack(predictions)
    predictions = scaler_y.inverse_transform(predictions)
    return predictions
# å®šä¹‰ä¸€æ®µé»˜è®¤ä»£ç 
DEFAULT_CODE_BP = """
# åŠ è½½å’Œå‡†å¤‡æ•°æ®
data = pd.read_csv('C://Users//Administrator//Desktop//ChatTTS-main//pages//earthquake.csv')
data['Date'] = pd.to_datetime(data['Date'])
data['Time'] = pd.to_datetime(data['Time'], format='%H:%M:%S').dt.time
data['Timestamp'] = data.apply(lambda row: pd.Timestamp(f"{row['Date']} {row['Time']}"), axis=1)
data['Timestamp'] = data['Timestamp'].view('int64') // 10**9 #Unixæ—¶é—´æˆ³
features = data[['Latitude', 'Longitude', 'Timestamp']]
targets = data[['Depth', 'Magnitude']]
X = torch.tensor(features.values, dtype=torch.float32)
Y = torch.tensor(targets.values, dtype=torch.float32)
dataset = TensorDataset(X, Y)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
# è®¡ç®—åˆ†å‰²å°ºå¯¸
test_size = int(len(dataset) * 0.2)  # 20%ä½œä¸ºæµ‹è¯•é›†
train_size = len(dataset) - test_size  # å‰©ä½™ä½œä¸ºè®­ç»ƒé›†

# éšæœºåˆ†å‰²æ•°æ®é›†
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

# ä½ å¯ä»¥å°†è¿™äº›æ•°æ®é›†è¿›ä¸€æ­¥å°è£…æˆDataLoaderï¼Œä¾¿äºæ‰¹å¤„ç†å’Œè¿­ä»£
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# ä»test_datasetä¸­æå–X_testå’ŒY_test
# æ³¨æ„ï¼šrandom_splitè¿”å›çš„å­é›†ç±»å‹æ˜¯Subsetï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡.datasetå±æ€§è®¿é—®åŸå§‹æ•°æ®
X_test = torch.stack([data[0] for data in test_dataset])
Y_test = torch.stack([data[1] for data in test_dataset])

if torch.cuda.is_available():
    device = torch.device("cuda:0")  # å¦‚æœæœ‰å¤šä¸ªGPUï¼Œå¯ä»¥æ›´æ”¹ç´¢å¼•ä» 0 åˆ° 1, 2, ...
    print("Training on GPU...")
else:
    device = torch.device("cpu")
    print("CUDA is not available. Training on CPU...")


# åˆå§‹åŒ–ç½‘ç»œ
model = ComplexNet()
model.to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)  # ä½¿ç”¨Adamä¼˜åŒ–å™¨
# è®­ç»ƒç½‘ç»œ
min_loss=10000
epochs = 200
for epoch in range(epochs):
    for batch_X, batch_Y in train_loader:
        batch_X, batch_Y = batch_X.to(device), batch_Y.to(device)  # ç§»åŠ¨åˆ°GPU

        optimizer.zero_grad()
        predictions = model(batch_X)
        loss = criterion(predictions, batch_Y)
        loss.backward()
        optimizer.step()
    

    print(f'Epoch {epoch+1}, Loss: {loss.item()}')
    if min_loss>loss.item():
        torch.save(model.state_dict(), 'minloss_BP_model.pth')
        min_loss=loss.item()
predictions = predictions.cpu()
# ä¿å­˜æ¨¡å‹
torch.save(model.state_dict(), 'lastloss_BP_model.pth')
"""
DEFAULT_CODE_LSTM = """
# åŠ è½½å’Œå‡†å¤‡æ•°æ®
data = pd.read_csv('C://Users//Administrator//Desktop//ChatTTS-main//pages//earthquake.csv')
data['Date'] = pd.to_datetime(data['Date'])
data['Time'] = pd.to_datetime(data['Time'], format='%H:%M:%S').dt.time
data['Timestamp'] = data.apply(lambda row: pd.Timestamp(f"{row['Date']} {row['Time']}"), axis=1)
data['Timestamp'] = data['Timestamp'].view('int64') // 10**9 #Unixæ—¶é—´æˆ³
features = data[['Timestamp', 'Longitude', 'Latitude']]
targets = data[['Depth', 'Magnitude']]

X_np=features.values
y_np=targets.values
scaler_x = MinMaxScaler()
scaler_y = MinMaxScaler()
scaler_x.fit(X_np)
scaler_y.fit(y_np)

# ä½¿ç”¨ç›¸åŒçš„å‚æ•°å¯¹è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®è¿›è¡Œå½’ä¸€åŒ–
X_normalized = scaler_x.transform(X_np)
y_normalized = scaler_y.transform(y_np)
def create_sliding_windows(X, Y, window_size):
    X_windows, Y_windows = [], []
    for i in range(len(X) - window_size):
        # ä»Xä¸­æå–çª—å£
        X_window = X[i:i+window_size]
        # ä»Yä¸­æå–ç´§æ¥ç€çª—å£çš„ä¸‹ä¸€ä¸ªå€¼ä½œä¸ºæ ‡ç­¾
        Y_window = Y[i+window_size]
        X_windows.append(X_window)
        Y_windows.append(Y_window)
    return np.array(X_windows), np.array(Y_windows)

window_size = 10
X_windows, Y_windows = create_sliding_windows(X_normalized, y_normalized, window_size)
X = torch.tensor(X_windows, dtype=torch.float32)
Y = torch.tensor(Y_windows, dtype=torch.float32)
# ç¡®å®šæµ‹è¯•é›†çš„å¤§å°
test_size = 0.2

# è®¡ç®—æµ‹è¯•é›†åº”æœ‰çš„æ•°æ®ç‚¹æ•°é‡
num_data_points = len(X)
num_test_points = int(num_data_points * test_size)

# è®¡ç®—æµ‹è¯•é›†å’Œè®­ç»ƒé›†çš„åˆ†å‰²ç‚¹
split_point = num_data_points - num_test_points

# æŒ‰é¡ºåºåˆ’åˆ†æ•°æ®ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train = X[:split_point]
X_test = X[split_point:]
y_train = Y[:split_point]
y_test = Y[split_point:]
# åˆ›å»ºDataLoader
train_data = TensorDataset(X_train, y_train)
test_data = TensorDataset(X_test, y_test)
batch_size=64
train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)
# åˆå§‹åŒ–æ¨¡å‹
input_dim = 3  # ç»´åº¦ã€ç»åº¦ã€æ—¶é—´
hidden_dim = 64
num_layers = 2
output_dim = 2  # éœ‡æ·±å’Œéœ‡çº§
dropout_prob = 0.5

model = ComplexLSTMModel(input_dim, hidden_dim, output_dim, num_layers, dropout_prob)

# æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# å°†æ¨¡å‹ç§»åˆ°GPU
model = model.to(device)
# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# è®¾ç½®è®­ç»ƒè½®æ•°
num_epochs = 200

# ç”¨äºä¿å­˜æœ€ä½³æ¨¡å‹çš„é€»è¾‘
best_loss = np.inf
best_model_path = 'best_model.pth'
# è®­ç»ƒæ¨¡å‹
for epoch in range(num_epochs):
    model.train() # ç¡®ä¿æ¨¡å‹å¤„äºè®­ç»ƒæ¨¡å¼
    for inputs, targets in train_loader:
        inputs, targets = inputs.to(device), targets.to(device) # ç§»åŠ¨æ•°æ®åˆ°GPU
        optimizer.zero_grad() # æ¸…é™¤è¿‡å¾€æ¢¯åº¦
        
        outputs = model(inputs) # å‰å‘ä¼ æ’­
        
        loss = criterion(outputs, targets) # è®¡ç®—æŸå¤±
        loss.backward() # åå‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦
        optimizer.step() # æ›´æ–°æƒé‡
    
    # æ‰“å°è®­ç»ƒè¿›åº¦

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if loss.item() < best_loss:
        best_loss = loss.item()
        torch.save(model.state_dict(), best_model_path)
"""
DEFAULT_CODE_Transformer = """
# åŠ è½½å’Œå‡†å¤‡æ•°æ®
data = pd.read_csv('C://Users//Administrator//Desktop//ChatTTS-main//pages//earthquake.csv')
data['Date'] = pd.to_datetime(data['Date'])
data['Time'] = pd.to_datetime(data['Time'], format='%H:%M:%S').dt.time
data['Timestamp'] = data.apply(lambda row: pd.Timestamp(f"{row['Date']} {row['Time']}"), axis=1)
data['Timestamp'] = data['Timestamp'].view('int64') // 10**9 #Unixæ—¶é—´æˆ³
features = data[['Timestamp', 'Longitude', 'Latitude']]
targets = data[['Depth', 'Magnitude']]
X_np=features.values
y_np=targets.values
scaler_x = MinMaxScaler()
scaler_y = MinMaxScaler()
scaler_x.fit(X_np)
scaler_y.fit(y_np)

# ä½¿ç”¨ç›¸åŒçš„å‚æ•°å¯¹è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®è¿›è¡Œå½’ä¸€åŒ–
X_normalized = scaler_x.transform(X_np)
y_normalized = scaler_y.transform(y_np)
def create_sliding_windows(X, Y, window_size):
    X_windows, Y_windows = [], []
    for i in range(len(X) - window_size):
        # ä»Xä¸­æå–çª—å£
        X_window = X[i:i+window_size]
        # ä»Yä¸­æå–ç´§æ¥ç€çª—å£çš„ä¸‹ä¸€ä¸ªå€¼ä½œä¸ºæ ‡ç­¾
        Y_window = Y[i+window_size]
        X_windows.append(X_window)
        Y_windows.append(Y_window)
    return np.array(X_windows), np.array(Y_windows)

window_size = 20
X_windows, Y_windows = create_sliding_windows(X_normalized, y_normalized, window_size)
X = torch.tensor(X_windows, dtype=torch.float32)
Y = torch.tensor(Y_windows, dtype=torch.float32)
# ç¡®å®šæµ‹è¯•é›†çš„å¤§å°
test_size = 0.2

# è®¡ç®—æµ‹è¯•é›†åº”æœ‰çš„æ•°æ®ç‚¹æ•°é‡
num_data_points = len(X)
num_test_points = int(num_data_points * test_size)

# è®¡ç®—æµ‹è¯•é›†å’Œè®­ç»ƒé›†çš„åˆ†å‰²ç‚¹
split_point = num_data_points - num_test_points

# æŒ‰é¡ºåºåˆ’åˆ†æ•°æ®ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train = X[:split_point]
X_test = X[split_point:]
y_train = Y[:split_point]
y_test = Y[split_point:]
# åˆ›å»ºDataLoader
train_data = TensorDataset(X_train, y_train)
test_data = TensorDataset(X_test, y_test)
batch_size=64
train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)
# åˆå§‹åŒ–æ¨¡å‹å‚æ•°
input_dim = 3  # æ—¶é—´ã€ç»åº¦ã€çº¬åº¦
seq_len = 20   # åºåˆ—é•¿åº¦
hidden_dim = 64  # éšè—å±‚çš„ç»´åº¦
num_layers = 2  # Transformerå±‚çš„æ•°é‡
output_dim = 2  # éœ‡æ·±å’Œéœ‡çº§
dropout_prob = 0.5  # Dropoutæ¦‚ç‡

# åˆ›å»ºæ¨¡å‹
model = TransformerModel(input_dim, seq_len, hidden_dim, output_dim, num_layers, dropout_prob)

# æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# å°†æ¨¡å‹ç§»åˆ°GPU
model = model.to(device)

# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.MSELoss()  # å› ä¸ºæ˜¯å›å½’é—®é¢˜
optimizer = optim.Adam(model.parameters(), lr=0.001)

# è®¾ç½®è®­ç»ƒè½®æ•°
num_epochs = 200
best_loss = np.inf
best_model_path = 'best_transformer_model.pth'

for epoch in range(num_epochs):
    model.train()  # ç¡®ä¿æ¨¡å‹å¤„äºè®­ç»ƒæ¨¡å¼
    epoch_loss = 0.0  # ç”¨äºç´¯ç§¯æ•´ä¸ªepochçš„æŸå¤±

    for inputs, targets in train_loader:
        inputs, targets = inputs.to(device), targets.to(device)  # ç§»åŠ¨æ•°æ®åˆ°GPU
        
        optimizer.zero_grad()  # æ¸…é™¤è¿‡å¾€æ¢¯åº¦
        
        outputs = model(inputs)  # å‰å‘ä¼ æ’­
        
        loss = criterion(outputs, targets)  # è®¡ç®—æŸå¤±
        loss.backward()  # åå‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦
        optimizer.step()  # æ›´æ–°æƒé‡

        epoch_loss += loss.item() * inputs.size(0)  # ç´¯ç§¯æŸå¤±

    # è®¡ç®—æ•´ä¸ªepochçš„å¹³å‡æŸå¤±
    epoch_loss /= len(train_loader.dataset)
    
    # æ‰“å°è®­ç»ƒè¿›åº¦

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if epoch_loss < best_loss:
        best_loss = epoch_loss
        torch.save(model.state_dict(), best_model_path)
torch.save(model.state_dict(), 'last_transformer_model.pth')


"""
def execute_code(code):
    # åˆ›å»ºä¸€ä¸ªå­—ç¬¦ä¸²æµæ¥æ•è·æ‰§è¡Œè¾“å‡º
    output = StringIO()
    with contextlib.redirect_stdout(output):
        with contextlib.redirect_stderr(output):
            try:
                # æ‰§è¡Œç”¨æˆ·æä¾›çš„ä»£ç 
                exec(code)
            except Exception as e:
                # å¦‚æœä»£ç æ‰§è¡Œä¸­å‡ºç°é”™è¯¯ï¼Œæ‰“å°é”™è¯¯ä¿¡æ¯
                print(f"Error: {e}")
    # è·å–è¾“å‡ºå¹¶è¿”å›
    return output.getvalue()

def main():
    st.title("ç¥ç»ç½‘ç»œé¢„æµ‹")


    # ç”¨æˆ·é€‰æ‹©æ¨¡å‹
    genre = st.radio(
        "Select prediction model:",
        ["âš¡ BP", "ğŸ” LSTM", "ğŸ¤– Transformer"],
        captions=["BP Neural Network Model", "LSTM Neural Network Model", "Transformer Neural Network Model"]
    )

    # è·å–ç”¨æˆ·è¾“å…¥çš„å‚æ•°
    time = st.text_input("Enter Time (YYYY-MM-DD HH:MM:SS)", "1983-01-02 13:44:18")

    Latitude = st.number_input("Enter Latitude", value=19.246, format="%.2f")
    Longitude = st.number_input("Enter Longitude", value=145.616, format="%.2f")
    # æ£€æŸ¥æ˜¯å¦å·²è¾“å…¥æ‰€æœ‰å‚æ•°ï¼Œå¹¶å®šä¹‰æŒ‰é’®è§¦å‘é¢„æµ‹
    depth=None
    magnitude=None
    a=None
    if st.button("Predict"):
        # æ ¹æ®é€‰æ‹©çš„æ¨¡å‹è°ƒç”¨ä¸åŒçš„å‡½æ•°
        if genre == "âš¡ BP":
            a = BP_forward(time, Latitude, Longitude)
            depth = a[0][0]
            magnitude = a[0][1]

        elif genre == "ğŸ” LSTM":
            a = lstm_forward(time, Latitude, Longitude)
            depth = a[0][0]
            magnitude = a[0][1]
        elif genre == "ğŸ¤– Transformer":
            a = transformer_forward(time, Latitude, Longitude)
            depth = a[0][0]
            magnitude = a[0][1]
        # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
        st.write(f"Predicted Depth: {depth} km")
        st.write(f"Predicted Magnitude: {magnitude}")
    else:
        st.write("Enter all parameters to get the prediction.")

    st.markdown('***BPäººå·¥ç¥ç»ç½‘ç»œè®­ç»ƒ***')
    # ä½¿ç”¨ streamlit-ace ç»„ä»¶è®©ç”¨æˆ·å¯ä»¥è¾“å…¥ä»£ç ï¼ŒåŒæ—¶å±•ç¤ºé»˜è®¤ä»£ç 
    code_bp = st_ace.st_ace(language='python', theme='monokai', value=DEFAULT_CODE_BP)
    if st.button("Execute Code BP"):
        # å½“ç”¨æˆ·ç‚¹å‡»æ‰§è¡Œæ—¶ï¼Œè°ƒç”¨ execute_code å‡½æ•°
        output = execute_code(code_bp)
        st.text_area("Output", output, height=300)
    st.markdown('***LSTM***')
    # ä½¿ç”¨ streamlit-ace ç»„ä»¶è®©ç”¨æˆ·å¯ä»¥è¾“å…¥ä»£ç ï¼ŒåŒæ—¶å±•ç¤ºé»˜è®¤ä»£ç 
    code_lstm = st_ace.st_ace(language='python', theme='monokai', value=DEFAULT_CODE_LSTM)
    if st.button("Execute Code LSTM"):
        # å½“ç”¨æˆ·ç‚¹å‡»æ‰§è¡Œæ—¶ï¼Œè°ƒç”¨ execute_code å‡½æ•°
        output = execute_code(code_lstm)
        st.text_area("Output", output, height=300)
    st.markdown('***Transformer***')
    # ä½¿ç”¨ streamlit-ace ç»„ä»¶è®©ç”¨æˆ·å¯ä»¥è¾“å…¥ä»£ç ï¼ŒåŒæ—¶å±•ç¤ºé»˜è®¤ä»£ç 
    code_Transformer = st_ace.st_ace(language='python', theme='monokai', value=DEFAULT_CODE_Transformer)
    if st.button("Execute Code Transformer"):
        # å½“ç”¨æˆ·ç‚¹å‡»æ‰§è¡Œæ—¶ï¼Œè°ƒç”¨ execute_code å‡½æ•°
        output = execute_code(code_Transformer)
        st.text_area("Output", output, height=300)

if __name__ == "__main__":
    main()
